{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai cohere tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "example_file = open(\"../json/example.json\",\"r\")\n",
    "test_file = open(\"../json/test.json\",\"r\")\n",
    "format = open(\"../json/format.json\",\"r\").read()\n",
    "tools = open(\"../json/tools.json\",\"r\").read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get examples\n",
    "test_examples = json.loads(test_file.read())\n",
    "examples = json.loads(example_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= \"\"\"\n",
    "{tools_template}\n",
    "\n",
    "{format_template}\n",
    "\n",
    "{example_template}\n",
    "\n",
    "{start_template}\n",
    "\"\"\"\n",
    "\n",
    "tools_template = \"\"\"\n",
    "Use the following tools and answer the given question using given answer format, \n",
    "{tools}\n",
    "\"\"\"\n",
    "answer_format_template = \"\"\"\n",
    "Generated answer for any given question should have following format\n",
    "{format}\n",
    "\"\"\"\n",
    "\n",
    "example_template = \"\"\"\n",
    "Following are the example of question-answer pairs for the abovementioned tools\n",
    "{example1}\n",
    "\"\"\"\n",
    "\n",
    "start_template = \"\"\"\n",
    "To reference the value of the ith tool in the chain, use $$PREV[i] as argument value. i = 0, 1, .. j-1; j = current tool's index in the array\n",
    "If the query could not be answered with the given set of tools, output an empty list instead.\n",
    "\n",
    "Empty list should look like []. Whenever you want to access information about current user to pass it further, you need to make a call to whoami api tool.\n",
    "\n",
    "Answer the following question based on instructions provided above.\n",
    "question: {question}\n",
    "answer:{{\n",
    "\"question\":\n",
    "\"answer\":\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example1', 'tools', 'format', 'question']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "full_prompt = PromptTemplate.from_template(prompt_template)\n",
    "tools_prompt = PromptTemplate.from_template(tools_template)\n",
    "answer_format_prompt = PromptTemplate.from_template(answer_format_template)\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "input_prompt = [\n",
    "    (\"tools_template\",tools_prompt),\n",
    "    (\"format_template\",answer_format_prompt),\n",
    "    (\"example_template\",example_prompt),\n",
    "    (\"start_template\",start_prompt)\n",
    "]\n",
    "prompt = PipelinePromptTemplate(final_prompt=full_prompt,pipeline_prompts=input_prompt)\n",
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /raid/test1/anaconda3/lib/python3.11/site-packages (1.3.5)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /raid/test1/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /raid/test1/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /raid/test1/anaconda3/lib/python3.11/site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /raid/test1/anaconda3/lib/python3.11/site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: tqdm>4 in /raid/test1/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /raid/test1/anaconda3/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in /raid/test1/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /raid/test1/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /raid/test1/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in /raid/test1/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /raid/test1/anaconda3/lib/python3.11/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "config = open(\"config.txt\",\"r\").read()\n",
    "OPENAI_API_KEY=config\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = ChatOpenAI(model='gpt-4'), prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "response = []\n",
    "for test in test_examples:\n",
    "\n",
    "    print(test['question_id'])\n",
    "    # input = prompt.format(question=test['question'],tools=tools,format=format,examples=examples)\n",
    "    output = chain.invoke({'question':test['question'],'tools':tools,'format':format,'example1':examples[0]})\n",
    "\n",
    "    answer = json.loads(output['text'])\n",
    "    if (answer == []):\n",
    "        answer = {'question':test['question'],'answer':[]}\n",
    "    answer['question_id'] = test['question_id']\n",
    "    response.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"../output/zero shot with one example/GPT_4_res.json\",\"w\")\n",
    "json.dump(response,outfile,indent=4)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
